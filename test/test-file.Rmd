---
title: "STAT525-HW9"
author: "Keming Li"
date: "10/26/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Hierarchical Poisson Model for Heart Surgery Data
## 1.
$$
\begin{split}
  &p(y_{h},\lambda|\alpha,\mu) = p(y_{h}|\lambda,\alpha,\mu)\cdot p(\lambda|\alpha,\mu)\\
  &= \frac{e^{-x_{h}\lambda_{h}}(x_{h}\lambda_{h})^{y_{h}}}{y_{h}!}\cdot \lambda_{h}^{\alpha-1}\cdot e^{-\lambda_{h}\cdot\alpha/\mu}\cdot\frac{(\alpha/\mu)^\alpha}{\Gamma(\alpha)}\\
  &= \frac{x_{h}^{y_{h}}}{y_{h}!}\frac{(\alpha/\mu)^\alpha}{\Gamma(\alpha)}\cdot\lambda_{h}^{y_{h}+\alpha-1}\cdot e^{-\lambda_{h}(x_{h}+\alpha/\mu)}\\
  &\therefore p(y_{h}|\alpha,\mu) = \int_{0}^\infty p(y_{h},\lambda|\alpha,\mu)d\lambda_{h}\\
  &= \frac{x_{h}^{y_{h}}}{y_{h}!}\frac{(\alpha/\mu)^\alpha}{\Gamma(\alpha)}\int_{0}^\infty \lambda_{h}^{y_{h}+\alpha-1}\cdot e^{-\lambda_{h}(x_{h}+\alpha/\mu)}\cdot \frac{(\alpha/\mu+x_{h})^\alpha}{\Gamma(\alpha+y_{h})}\cdot \frac{\Gamma(\alpha+y_{h})}{(\alpha/\mu+x_{h})^\alpha}d\lambda_{h}\\
  &= \frac{x_{h}^{y_{h}}}{y_{h}!}\frac{(\alpha/\mu)^\alpha}{\Gamma(\alpha)}\frac{\Gamma(\alpha+y_{h})}{(\alpha/\mu+x_{h})^\alpha}
\end{split}
$$

## 2.
### (1.)
```{r, echo=TRUE, results=TRUE}
rm(list = ls())
library(LearnBayes)
library(MASS)
library(coda)
data("hearttransplants")
# Number of hospitals:
H = length(hearttransplants$y)
# Response (y) and exposure (x):
y = hearttransplants$y
x = hearttransplants$e

# Hyperparameter for alpha:
z_0 = 1/2

# Function to compute the posterior
## theta = (mu, alpha)
post = function(theta){
prod(x^y/factorial(y) * ((theta[2]/theta[1])^theta[2]/gamma(theta[2])) *
gamma(theta[2] + y)/(x + theta[2]/theta[1])^(theta[2] + y)) *
1/theta[1] * z_0/(z_0 + theta[2])^2
}
# Function to compute the log-posterior
log_post = function(theta){
sum(log(x^y/factorial(y)) + log((theta[2]/theta[1])^theta[2]/gamma(theta[2])) +
log(gamma(theta[2] + y)/(x + theta[2]/theta[1])^(theta[2] + y))) +
log(1/theta[1]) + log(z_0/(z_0 + theta[2])^2)
}
#---------------------------------------------------------------------------
## Metropolis-Hastings w/ RANDOM WALK Re-estimate Proposal
#---------------------------------------------------------------------------
S = 10^4
# Initial values:
theta = c(mean(y/x), 5)
# Choose a proposal variance:
#Sigma_prop = diag(2)
#Sigma_prop = var(post_theta)
Sigma_prop = matrix(c(6.502520e-09, -6.516817e-05,
                      -6.516817e-05, 4.141172e+01), nrow = 2)
# Count accepted:
count_accept = 0
# Store posterior simulations:
post_theta = array(0, c(S, 2))
for(s in 1:S){
# Random walk proposal:
repeat{
theta_star = mvrnorm(n = 1,
                     mu = theta, # random walk part
                     Sigma = Sigma_prop)
if(theta_star[1] > 0 &
theta_star[2] > 0 &
post(theta) != 0 &
!is.na(post(theta_star))){
break
}
}
# Compute r: (post is more stable than log_post here)
r = post(theta_star)/post(theta)
# Options to accept:
if(runif(1) < r){
theta = theta_star;
count_accept = count_accept + 1
}
#If we donâ€˜t accept, do nothing!
post_theta[s,] = theta
}
Sigma_prop
count_accept/S
```

So after rerun the MCMC reveral times, we can get the Sigma_prop result:
$$\left( \begin{array}{cc} 6.502520e^{-09} & -6.516817e^{-05} \\
-6.516817e^{-05} & 4.141172e^{+01} \end{array} \right)$$

### (2.)
```{r, echo=TRUE, results=TRUE}
post_lambda_h = array(0, c(S, H)) 
post_kappa_h = array(0, c(S, H))
for (s in 1:S){ for (h in 1:H){
    # sample lambda_h
lambda_h = rgamma(1, shape = post_theta[s,2] + y[h],
rate = x[h] + post_theta[s,2]/post_theta[s,1])
    # Store MCMC output:
    post_lambda_h[s,h] = lambda_h
  }
  # Store MCMC output:
post_kappa_h[s,] = post_theta[s,2]/(post_theta[s,2] + post_theta[s,1]*x)
}
```

## 3.
```{r, echo=TRUE, results=TRUE}
# plot the trace plot for mu and alpha
plot(as.mcmc(post_theta))
# plot the trace plot for lambda_1
plot(as.mcmc(post_lambda_h[,1]))
# plot the trace plot for lambda_80
plot(as.mcmc(post_lambda_h[,80]))
# How many "independent samples" do we have for lambda_1
effectiveSize(as.mcmc(post_lambda_h[,1]))
# How many "independent samples" do we have for lambda_80
effectiveSize(as.mcmc(post_lambda_h[,80]))
# How many "independent samples" do we have for mu and alpha
effectiveSize(as.mcmc(post_theta))
```

From the results of effectivesize, we can see that we've already use the large enough sample size, so the MCMC efficient. Also, from the trace plots aove ,we can see that the chain is converged.

## 4.
```{r, echo=TRUE, results=TRUE}
mean_kappa = apply(post_kappa_h,2,mean)
#Plot these posterior means as a function of logx_h
plot(log(x),apply(post_kappa_h,2,mean))
```

So from the plot above, we can see that the posterior means would decrease from 1 to 0.4 when the log(x) increases.

## 5.
```{r, echo=TRUE, results=TRUE}
# HPD intervals:
ci_h = HPDinterval(as.mcmc(post_lambda_h))
# Unpooled MLE:
yhat_mle = y/x
# Pooled MLE:
yhat_mle_pool = sum(y)/sum(x)
plot(log(x), yhat_mle, ylim = range(ci_h, y/x),
pch = as.character(y), cex = 1.5,
main = 'Death rates by (log) Exposure')
abline(h = yhat_mle_pool, lwd=4, col='green', lty=2)
lines(log(x), apply(post_lambda_h, 2, mean),
      col = 'red', type = 'p')
for(h in 1:H){ lines(rep(log(x[h]), 2),
        ci_h[h,], col='blue')
}
```

So from the graphic above, we can see that the unpooled MlE would have a lot of difference when h change. But when the model is completely pooled, the MLE would be totally same when h change. For this partially pooled model, the MLE is fluctuated around the line of completely pooled situation as these red circle shown above.

# 2. Hierarchical Model and multiple comparisons
```{r, echo=TRUE, results=TRUE}
rm(list = ls()); library(coda)
y_j = c(28, 8, -3, 7, -1, 1, 18, 12)
sigma_j = c(15, 10, 16, 11, 9, 11, 10, 18)
names(y_j) = names(sigma_j) = c("A", "B", "C", "D", "E","F", "G", "H")
J = length(y_j)

theta_j = y_j
mu = mean(theta_j)
sigma_theta = sd(y_j) 

# Scale hyperprior for sigma_theta:
library(truncdist) # For truncated sampler
A = 100  # Upper bound on SD
S = 10^4

post_theta_j = array(0, c(S, J))
post_mu = post_sigma_theta = numeric(S)
post_kappa_j = array(0, c(S,J))
for(s in 1:S){
  # Block 1: sample mu (marg. over theta!)
  Q_mu = sum(1/(sigma_j^2 + sigma_theta^2))
  ell_mu = sum(y_j/(sigma_j^2 + sigma_theta^2))
  mu = rnorm(n = 1,
             mean = Q_mu^-1*ell_mu,
             sd = sqrt(Q_mu^-1))
  
  # Block 2: sample theta_j:
  Q_theta = 1/sigma_j^2 + 1/sigma_theta^2
  ell_theta = y_j/sigma_j^2 + mu/sigma_theta^2
  theta_j = rnorm(n = J,
                  mean = Q_theta^-1*ell_theta,
                  sd = sqrt(Q_theta^-1))
  
  # Block 3: sample sigma_theta ~ Uniform(0, A)
  #eta_theta = rgamma(n = 1, 
  #                   shape = J/2 - 1/2,
  #                   rate = sum((theta_j-mu)^2)/2)
  eta_theta = rtrunc(n = 1, 
                     'gamma',   # Family of distribution
                     a = 1/A^2, # Lower interval
                     b = Inf,   # Upper interval
                     shape = J/2 - 1/2,
                     rate =  sum((theta_j-mu)^2)/2)
  sigma_theta = 1/sqrt(eta_theta)

  post_mu[s] = mu
  post_theta_j[s,] = theta_j
  post_sigma_theta[s] = sigma_theta
  
  # Shrinkage parameter:
  post_kappa_j[s, ] = sigma_j^2/(sigma_theta^2 + sigma_j^2)
}
effectiveSize(post_mu)
effectiveSize(post_theta_j)
effectiveSize(post_sigma_theta)
```

## 1.
### (a.)
#### (1.)
```{r, echo=TRUE, results=TRUE}
prob_j = array(J)
for (j in 1:J){
  count = 0
  for (s in 1:S){
    if (which.max(post_theta_j[s,]) == j) count = count + 1 
    }
prob_j[j] = count/S 
}
prob_j
```

#### (2.)
```{r, echo=TRUE, results=TRUE}
prob_jk = array(0,c(J,J)) 
for (j in 1:J){
  for (k in 1:J){ 
    count = 0
    for (s in 1:S){
      if (post_theta_j[s,j] > post_theta_j[s,k]) count = count + 1 
    }
    prob_jk[j,k] = count/S 
    }
} 
prob_jk
```

### (b.)
```{r, echo=TRUE, results=TRUE}
sigma_theta = sd(y_j)*100

post_theta_j2 = array(0, c(S, J))
post_mu2 = post_sigma_theta = numeric(S)
post_kappa_j2 = array(0, c(S,J))
for(s in 1:S){
  # Block 1: sample mu (marg. over theta!)
  Q_mu = sum(1/(sigma_j^2 + sigma_theta^2))
  ell_mu = sum(y_j/(sigma_j^2 + sigma_theta^2))
  mu = rnorm(n = 1,
             mean = Q_mu^-1*ell_mu,
             sd = sqrt(Q_mu^-1))
  
  # Block 2: sample theta_j:
  Q_theta = 1/sigma_j^2 + 1/sigma_theta^2
  ell_theta = y_j/sigma_j^2 + mu/sigma_theta^2
  theta_j = rnorm(n = J,
                  mean = Q_theta^-1*ell_theta,
                  sd = sqrt(Q_theta^-1))
  
  # Block 3: sample sigma_theta ~ Uniform(0, A)
  #eta_theta = rgamma(n = 1, 
  #                   shape = J/2 - 1/2,
  #                   rate = sum((theta_j-mu)^2)/2)
  eta_theta = rtrunc(n = 1, 
                     'gamma',   # Family of distribution
                     a = 1/A^2, # Lower interval
                     b = Inf,   # Upper interval
                     shape = J/2 - 1/2,
                     rate =  sum((theta_j-mu)^2)/2)
  sigma_theta = 1/sqrt(eta_theta)

  post_mu2[s] = mu
  post_theta_j2[s,] = theta_j
  post_sigma_theta[s] = sigma_theta
  
  # Shrinkage parameter:
  post_kappa_j2[s, ] = sigma_j^2/(sigma_theta^2 + sigma_j^2)
}
```

#### (1.)
```{r, echo=TRUE, results=TRUE}
prob_j2 = array(J)
for (j in 1:J){
  count = 0
  for (s in 1:S){
    if (which.max(post_theta_j2[s,]) == j) count = count + 1 
    }
prob_j2[j] = count/S 
}
prob_j2
```

#### (2.)
```{r, echo=TRUE, results=TRUE}
prob_jk2 = array(0,c(J,J)) 
for (j in 1:J){
  for (k in 1:J){ 
    count = 0
    for (s in 1:S){
      if (post_theta_j2[s,j] > post_theta_j2[s,k]) count = count + 1 
    }
    prob_jk2[j,k] = count/S 
    }
} 
prob_jk2
```

### (c.)
As $\tau$ goes to infinity, both the probabilities of of two partsare larger than before, which may lead to a positive miscoliboration. But the differences are not big, which means the p-values aren't affected by the infinite $\tau$ too much. I think that is because J = 8 in this case and is larger than 3.

### (d.)
As $\tau$ goes to 0, the theta_j would go to be constant. So the p-values also gonna be constant.

## 2.
```{r, echo=TRUE, results=TRUE}
alpha = .05 
prob_jk_p = array(0,c(J,J)) 
z_test = array(0,c(J,J)) 
for (j in 1:J){
  for (k in 1:J){
    z = mean(post_theta_j[,j] - post_theta_j[,k])/sigma_j[j] 
    z_test[j,k] = z
    prob_jk_p[j,k] = pnorm(z)
  } 
}
#z_test
prob_jk_p
```

So from the p-values, we need to accept the null hypothesis, which is theta_j = theta_k. This conclusion also corresponding to the result of Excercise 3(a)(ii), because all of the p-values are around 0.5 in the results of Excercise 3(a)(ii). And I think that is because theta_j and theta_k can be regared as equal statistically.

## 3.
```{r, echo=TRUE, results=TRUE}
prob_jk_p_bon = prob_jk_p*choose(J,2)
prob_jk_p_bon[prob_jk_p_bon > 1] = 1 
prob_jk_p_bon
```

So from this result, we can clearly see that we can accept the null hypothesis, which is theta_j = theta_k. This conclusion also corresponding to the result of Excercise 3(a)(ii).